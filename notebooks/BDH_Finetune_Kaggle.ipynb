{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc08113f",
   "metadata": {},
   "source": [
    "# BDH Full Pipeline: Train → Merge → Fine-tune (Kaggle GPU)\n",
    "\n",
    "**Purpose:** Train better specialists, merge them, and fine-tune with frozen sparse layers so that:\n",
    "- Specialist losses reach ~0.5-0.7 (down from ~0.9)\n",
    "- Merged fine-tuned loss drops to ~1.0-1.5 on both languages\n",
    "- Heritage probe shows clear language routing (French input → French neurons)\n",
    "\n",
    "**What this notebook does:**\n",
    "1. Retrain French & Portuguese specialists for **15,000 iterations** (was 5,000)\n",
    "2. Merge the two specialists (concatenate neurons)\n",
    "3. Fine-tune with `--freeze-sparse` to fix embed/lm_head while **preserving specialist neuron identity**\n",
    "4. Evaluate all 4 models + heritage probe → produce `merge_data.json`\n",
    "\n",
    "**Settings:** Kaggle → GPU T4 x2, Internet ON\n",
    "\n",
    "**Time estimate:** ~60-80 min total (retrain ~40 min + download + finetune + eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce077e8",
   "metadata": {},
   "source": [
    "## Step 0: Verify GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec442462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    mem = getattr(props, 'total_memory', None) or getattr(props, 'total_mem', 0)\n",
    "    print(f\"Memory: {mem / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected! This will be very slow on CPU.\")\n",
    "    print(\"Go to Settings > Accelerator > GPU T4 x2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ee4911",
   "metadata": {},
   "source": [
    "## Step 1: Setup Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3ee73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil, glob, subprocess\n",
    "\n",
    "# Find project in Kaggle input\n",
    "INPUT_BASE = \"/kaggle/input\"\n",
    "WORK_DIR = \"/kaggle/working/bdh\"\n",
    "\n",
    "# ── 1. Diagnostics: what's actually in /kaggle/input? ──\n",
    "print(\"Contents of /kaggle/input:\")\n",
    "if os.path.exists(INPUT_BASE):\n",
    "    top = os.listdir(INPUT_BASE)\n",
    "    if not top:\n",
    "        print(\"  (empty!)\")\n",
    "        print(\"\\n⚠ You need to attach the dataset to this notebook:\")\n",
    "        print(\"  1. Click 'Add Data' in the right sidebar\")\n",
    "        print(\"  2. Search for your uploaded dataset name\")\n",
    "        print(\"  3. Click 'Add' to attach it\")\n",
    "        print(\"  4. Re-run this cell\\n\")\n",
    "    else:\n",
    "        for item in top:\n",
    "            full = os.path.join(INPUT_BASE, item)\n",
    "            kind = \"dir\" if os.path.isdir(full) else f\"file ({os.path.getsize(full)/1e6:.1f} MB)\"\n",
    "            print(f\"  {item}  [{kind}]\")\n",
    "else:\n",
    "    print(\"  /kaggle/input does not exist!\")\n",
    "\n",
    "# ── 2. If there's a zip file, extract it first ──\n",
    "for root, dirs, files in os.walk(INPUT_BASE):\n",
    "    for f in files:\n",
    "        if f.endswith('.zip'):\n",
    "            zpath = os.path.join(root, f)\n",
    "            extract_to = os.path.join(root, f.replace('.zip', '_extracted'))\n",
    "            if not os.path.exists(extract_to):\n",
    "                print(f\"\\nFound zip: {zpath} — extracting...\")\n",
    "                import zipfile\n",
    "                with zipfile.ZipFile(zpath, 'r') as zf:\n",
    "                    zf.extractall(extract_to)\n",
    "                print(f\"  Extracted to: {extract_to}\")\n",
    "    depth = root.replace(INPUT_BASE, '').count(os.sep)\n",
    "    if depth >= 2:\n",
    "        dirs.clear()\n",
    "\n",
    "# ── 3. Search for project root (contains training/bdh.py) ──\n",
    "src = None\n",
    "for root, dirs, files in os.walk(INPUT_BASE):\n",
    "    if 'training' in dirs:\n",
    "        bdh_check = os.path.join(root, 'training', 'bdh.py')\n",
    "        if os.path.exists(bdh_check):\n",
    "            src = root\n",
    "            break\n",
    "    depth = root.replace(INPUT_BASE, '').count(os.sep)\n",
    "    if depth >= 6:\n",
    "        dirs.clear()\n",
    "\n",
    "if src is None:\n",
    "    print(\"\\n❌ ERROR: Could not find BDH project (training/bdh.py) in /kaggle/input/\")\n",
    "    print(\"\\nFull directory listing:\")\n",
    "    for root, dirs, files in os.walk(INPUT_BASE):\n",
    "        depth = root.replace(INPUT_BASE, '').count(os.sep)\n",
    "        indent = \"  \" * depth\n",
    "        print(f\"{indent}{os.path.basename(root)}/\")\n",
    "        if depth >= 4:\n",
    "            dirs.clear()\n",
    "        for f in files[:8]:\n",
    "            print(f\"{indent}  {f}\")\n",
    "        if len(files) > 8:\n",
    "            print(f\"{indent}  ... and {len(files)-8} more files\")\n",
    "    raise RuntimeError(\"Dataset not found — see instructions above\")\n",
    "else:\n",
    "    print(f\"\\n✓ Found project: {src}\")\n",
    "    if os.path.exists(WORK_DIR):\n",
    "        shutil.rmtree(WORK_DIR)\n",
    "    shutil.copytree(src, WORK_DIR)\n",
    "    os.chdir(WORK_DIR)\n",
    "    print(f\"  Copied to: {WORK_DIR}\")\n",
    "    print(f\"  Top-level: {sorted(os.listdir('.'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16220494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch bdh.py for PyTorch 2.6+ (weights_only=False)\n",
    "import re\n",
    "bdh_path = os.path.join(WORK_DIR, 'training', 'bdh.py')\n",
    "with open(bdh_path, 'r') as f:\n",
    "    content = f.read()\n",
    "\n",
    "if 'weights_only' not in content:\n",
    "    patched = content.replace(\n",
    "        'torch.load(path, map_location=device)',\n",
    "        'torch.load(path, map_location=device, weights_only=False)'\n",
    "    )\n",
    "    with open(bdh_path, 'w') as f:\n",
    "        f.write(patched)\n",
    "    print('Patched bdh.py for PyTorch 2.6+')\n",
    "else:\n",
    "    print('bdh.py already patched')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82441ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pyyaml tqdm requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24a3d8a",
   "metadata": {},
   "source": [
    "## Step 2: Verify Checkpoints Exist\n",
    "\n",
    "The merged checkpoint must already exist from the Pipeline notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbca327",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check for checkpoints - try multiple known paths\n",
    "MERGED_CANDIDATES = [\n",
    "    \"checkpoints/merged_polyglot/checkpoint_best.pt\",\n",
    "    \"checkpoints/merged_polyglot.pt\",\n",
    "]\n",
    "FRENCH_CANDIDATES = [\n",
    "    \"checkpoints/french_specialist/checkpoint_best.pt\",\n",
    "    \"checkpoints/french/french_best.pt\",\n",
    "]\n",
    "PORTUGUESE_CANDIDATES = [\n",
    "    \"checkpoints/portuguese_specialist/checkpoint_best.pt\",\n",
    "]\n",
    "\n",
    "def find_first(candidates, label):\n",
    "    for c in candidates:\n",
    "        if os.path.exists(c):\n",
    "            print(f\"  {label}: {c} ({os.path.getsize(c)/1e6:.1f} MB)\")\n",
    "            return c\n",
    "    print(f\"  {label}: NOT FOUND (tried {candidates})\")\n",
    "    return None\n",
    "\n",
    "MERGED_PATH = find_first(MERGED_CANDIDATES, \"Merged\")\n",
    "FRENCH_PATH = find_first(FRENCH_CANDIDATES, \"French\")\n",
    "PORTUGUESE_PATH = find_first(PORTUGUESE_CANDIDATES, \"Portuguese\")\n",
    "\n",
    "if not MERGED_PATH:\n",
    "    print(\"\\nERROR: No merged checkpoint found!\")\n",
    "    print(\"You need to run the BDH_Kaggle_Pipeline notebook first.\")\n",
    "    print(\"\\nOr if specialists exist, we can merge now:\")\n",
    "    if FRENCH_PATH and PORTUGUESE_PATH:\n",
    "        print(\"  Specialists found! Will merge in next cell.\")\n",
    "        NEED_MERGE = True\n",
    "    else:\n",
    "        print(\"  No specialists either. Run Pipeline notebook first.\")\n",
    "        NEED_MERGE = False\n",
    "else:\n",
    "    NEED_MERGE = False\n",
    "    print(\"\\nAll checkpoints found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8ed2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge if needed (only runs if merged checkpoint was missing)\n",
    "if NEED_MERGE and FRENCH_PATH and PORTUGUESE_PATH:\n",
    "    MERGED_PATH = \"checkpoints/merged_polyglot/checkpoint_best.pt\"\n",
    "    os.makedirs(os.path.dirname(MERGED_PATH), exist_ok=True)\n",
    "    !python analysis/merge.py \\\n",
    "        --model1 {FRENCH_PATH} \\\n",
    "        --model2 {PORTUGUESE_PATH} \\\n",
    "        --output {MERGED_PATH} \\\n",
    "        --skip-eval\n",
    "    print(f\"Merged checkpoint created: {MERGED_PATH}\")\n",
    "elif not MERGED_PATH:\n",
    "    raise RuntimeError(\"No merged checkpoint and can't create one. Run Pipeline first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dc21c0",
   "metadata": {},
   "source": [
    "## Step 3: Download Europarl Data\n",
    "\n",
    "We need the **real** Europarl .bin files for both training and fine-tuning.\n",
    "This downloads ~200MB of parallel corpus for both language pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41e3a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if data already exists (from Pipeline run)\n",
    "FR_TRAIN = \"data/en-fr/train.bin\"\n",
    "FR_VAL   = \"data/en-fr/val.bin\"\n",
    "PT_TRAIN = \"data/en-pt/train.bin\"\n",
    "PT_VAL   = \"data/en-pt/val.bin\"\n",
    "\n",
    "have_data = all(os.path.exists(p) for p in [FR_TRAIN, FR_VAL, PT_TRAIN, PT_VAL])\n",
    "\n",
    "if have_data:\n",
    "    for p in [FR_TRAIN, FR_VAL, PT_TRAIN, PT_VAL]:\n",
    "        print(f\"  {p}: {os.path.getsize(p)/1e6:.1f} MB\")\n",
    "    print(\"\\nData already exists!\")\n",
    "else:\n",
    "    print(\"Downloading Europarl data (this takes 5-10 min)...\")\n",
    "    !python training/download_europarl.py --languages en-fr en-pt --output data/\n",
    "    # Verify\n",
    "    for p in [FR_TRAIN, FR_VAL, PT_TRAIN, PT_VAL]:\n",
    "        if os.path.exists(p):\n",
    "            print(f\"  {p}: {os.path.getsize(p)/1e6:.1f} MB\")\n",
    "        else:\n",
    "            print(f\"  {p}: MISSING!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b928aeb",
   "metadata": {},
   "source": [
    "## Step 4: Retrain Specialists (15,000 iterations)\n",
    "\n",
    "The original Pipeline trained for only 5,000 iterations → specialist loss ~0.9.\n",
    "Training for 15,000 iterations should reach ~0.5-0.7 loss.\n",
    "\n",
    "Optimized for T4: `float16` + `batch_size=64` + `grad_accum=1` + `torch.compile`\n",
    "- **~10-15 min per specialist** (plus ~2 min one-time compile overhead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705184ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml, os\n",
    "\n",
    "# Optimized config for T4 GPU:\n",
    "#   - float16 (T4 has no bfloat16 tensor cores)\n",
    "#   - batch_size=64 (model is tiny, T4 has 16GB)\n",
    "#   - grad_accum=1 (no need with large batch)\n",
    "#   - torch.compile=True (one-time 2-3 min compile, then 1.5x faster)\n",
    "french_config = {\n",
    "    'train_data': 'data/en-fr/train.bin',\n",
    "    'val_data': 'data/en-fr/val.bin',\n",
    "    'n_layer': 6, 'n_embd': 192, 'n_head': 4,\n",
    "    'mlp_multiplier': 64, 'dropout': 0.1, 'vocab_size': 256,\n",
    "    'batch_size': 64, 'block_size': 256,\n",
    "    'max_iters': 15000,\n",
    "    'learning_rate': 1.0e-3, 'min_lr': 1.0e-4,\n",
    "    'warmup_iters': 500, 'weight_decay': 0.1,\n",
    "    'grad_clip': 1.0, 'gradient_accumulation_steps': 1,\n",
    "    'log_interval': 100, 'eval_interval': 1000,\n",
    "    'save_interval': 5000, 'eval_iters': 100,\n",
    "    'output_dir': 'checkpoints', 'run_name': 'french_specialist',\n",
    "    'device': 'cuda', 'dtype': 'float16', 'compile_model': True,\n",
    "}\n",
    "\n",
    "portuguese_config = french_config.copy()\n",
    "portuguese_config.update({\n",
    "    'train_data': 'data/en-pt/train.bin',\n",
    "    'val_data': 'data/en-pt/val.bin',\n",
    "    'run_name': 'portuguese_specialist',\n",
    "})\n",
    "\n",
    "os.makedirs('training/configs', exist_ok=True)\n",
    "with open('training/configs/french_15k.yaml', 'w') as f:\n",
    "    yaml.dump(french_config, f)\n",
    "with open('training/configs/portuguese_15k.yaml', 'w') as f:\n",
    "    yaml.dump(portuguese_config, f)\n",
    "\n",
    "tokens_per_iter = french_config['batch_size'] * french_config['block_size']\n",
    "print(f\"Configs written: 15,000 iters, 6L 192D 4H, mlp_multiplier=64\")\n",
    "print(f\"  dtype: float16 (T4 tensor cores)\")\n",
    "print(f\"  batch_size: {french_config['batch_size']}, grad_accum: {french_config['gradient_accumulation_steps']}\")\n",
    "print(f\"  Tokens/iter: {tokens_per_iter:,}\")\n",
    "print(f\"  torch.compile: {french_config['compile_model']}\")\n",
    "print(f\"  Est. ~10-15 min per specialist (after ~2 min compile)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9acad2a",
   "metadata": {},
   "source": [
    "## Step 4b: Train French Specialist (~12 min + 2 min compile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57888701",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python training/train.py --config training/configs/french_15k.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53bde09",
   "metadata": {},
   "source": [
    "## Step 4c: Train Portuguese Specialist (~12 min, compile cached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bde8b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python training/train.py --config training/configs/portuguese_15k.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b569c6fb",
   "metadata": {},
   "source": [
    "## Step 5: Merge the Retrained Specialists\n",
    "\n",
    "Concatenate neuron spaces: French(N=3072) + Portuguese(N=3072) → Merged(N=6144)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d1f423",
   "metadata": {},
   "outputs": [],
   "source": [
    "FRENCH_PATH = \"checkpoints/french_specialist/checkpoint_best.pt\"\n",
    "PORTUGUESE_PATH = \"checkpoints/portuguese_specialist/checkpoint_best.pt\"\n",
    "MERGED_PATH = \"checkpoints/merged_polyglot/checkpoint_best.pt\"\n",
    "\n",
    "import os\n",
    "os.makedirs(os.path.dirname(MERGED_PATH), exist_ok=True)\n",
    "\n",
    "# --skip-eval: we'll do full eval in Step 7 after fine-tuning\n",
    "!python analysis/merge.py \\\n",
    "    --model1 {FRENCH_PATH} \\\n",
    "    --model2 {PORTUGUESE_PATH} \\\n",
    "    --output {MERGED_PATH} \\\n",
    "    --skip-eval \\\n",
    "    --device cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bf45ed",
   "metadata": {},
   "source": [
    "## Step 6: Fine-tune with Frozen Sparse Layers\n",
    "\n",
    "Key: `--freeze-sparse` freezes the concatenated specialist neurons (encoder/decoder/freqs)\n",
    "and only trains `embed.weight` + `lm_head` (the averaged shared layers that need routing adaptation).\n",
    "\n",
    "This preserves specialist neuron identity → heritage probe shows proper language routing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4489c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINETUNED_PATH = \"checkpoints/merged_finetuned/checkpoint_best.pt\"\n",
    "\n",
    "import os\n",
    "os.makedirs(os.path.dirname(FINETUNED_PATH), exist_ok=True)\n",
    "\n",
    "!python analysis/finetune_merged.py \\\n",
    "    --checkpoint {MERGED_PATH} \\\n",
    "    --output {FINETUNED_PATH} \\\n",
    "    --french-data {FR_TRAIN} \\\n",
    "    --portuguese-data {PT_TRAIN} \\\n",
    "    --iters 1000 \\\n",
    "    --lr 1e-4 \\\n",
    "    --batch-size 4 \\\n",
    "    --block-size 256 \\\n",
    "    --freeze-sparse \\\n",
    "    --device cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a673a9c",
   "metadata": {},
   "source": [
    "## Step 7: Full Evaluation + Heritage Probe\n",
    "\n",
    "Run `merge.py` with `--skip-merge` to evaluate all 4 models and run the heritage probe.\n",
    "This produces the `merge_data.json` for the frontend visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29c3828",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python analysis/merge.py \\\n",
    "    --model1 {FRENCH_PATH} \\\n",
    "    --model2 {PORTUGUESE_PATH} \\\n",
    "    --output {MERGED_PATH} \\\n",
    "    --finetuned {FINETUNED_PATH} \\\n",
    "    --french-val {FR_VAL} \\\n",
    "    --portuguese-val {PT_VAL} \\\n",
    "    --skip-merge \\\n",
    "    --device cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5816cc0",
   "metadata": {},
   "source": [
    "## Step 8: Inspect Results\n",
    "\n",
    "Parse the JSON to see the 4-model narrative and heritage probe routing quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5c8ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the evaluation JSON\n",
    "json_path = \"frontend/public/merge/merge_data.json\"\n",
    "with open(json_path) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Model losses\n",
    "print(\"=\" * 60)\n",
    "print(\"  MODEL EVALUATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "for m in data.get(\"models\", []):\n",
    "    fr = m.get(\"french_loss\", \"N/A\")\n",
    "    pt = m.get(\"portuguese_loss\", \"N/A\")\n",
    "    fr_str = f\"{fr:.4f}\" if isinstance(fr, (int, float)) else str(fr)\n",
    "    pt_str = f\"{pt:.4f}\" if isinstance(pt, (int, float)) else str(pt)\n",
    "    print(f\"  {m['name']:30s}  FR={fr_str}  PT={pt_str}\")\n",
    "\n",
    "# Heritage probe\n",
    "probe = data.get(\"probe_data\", {})\n",
    "if probe:\n",
    "    summary = probe.get(\"summary\", {})\n",
    "    print(f\"\\n  Heritage Probe:\")\n",
    "    print(f\"    French input  → FR neurons {summary.get('french_input_french_pct', '?'):.1f}%\")\n",
    "    print(f\"    Portuguese input → PT neurons {summary.get('portuguese_input_portuguese_pct', '?'):.1f}%\")\n",
    "    rq = summary.get(\"routing_quality\", 0)\n",
    "    print(f\"    Routing quality: {rq:.1f}%\")\n",
    "    if rq > 60:\n",
    "        print(\"    ✓ Heritage probe shows language routing!\")\n",
    "    else:\n",
    "        print(\"    ✗ Probe shows weak routing — consider more freeze-sparse iters\")\n",
    "else:\n",
    "    print(\"\\n  No heritage probe data found in JSON\")\n",
    "\n",
    "# Finetune info\n",
    "fi = data.get(\"finetune_info\", {})\n",
    "if fi:\n",
    "    print(f\"\\n  Fine-tune info:\")\n",
    "    print(f\"    Iterations: {fi.get('iters', '?')}\")\n",
    "    print(f\"    Loss: {fi.get('pre_loss', '?')} → {fi.get('post_loss', '?')}\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f8a711",
   "metadata": {},
   "source": [
    "## Step 9: Quick Generation Sanity Check\n",
    "\n",
    "Generate some text from the fine-tuned model to visually verify output quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd0e8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, torch\n",
    "sys.path.insert(0, 'training')\n",
    "from bdh import load_model\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = load_model(FINETUNED_PATH, device)\n",
    "\n",
    "prompts = [\n",
    "    (\"French\", \"Le parlement européen a adopté\"),\n",
    "    (\"Portuguese\", \"O parlamento europeu adoptou\"),\n",
    "    (\"English\", \"The European Parliament has adopted\"),\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"  GENERATION SAMPLES (fine-tuned model)\")\n",
    "print(\"=\" * 60)\n",
    "for label, prompt in prompts:\n",
    "    tokens = torch.tensor([list(prompt.encode('utf-8'))], dtype=torch.long, device=device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(tokens, max_new_tokens=100, top_k=5, temperature=0.8)\n",
    "    text = bytes(out[0].cpu().tolist()).decode('utf-8', errors='replace')\n",
    "    print(f\"\\n  [{label}]\")\n",
    "    print(f\"  {text[:200]}\")\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d929f2ee",
   "metadata": {},
   "source": [
    "## Step 10: Package Results for Download\n",
    "\n",
    "Create a zip with the key outputs:\n",
    "- `merge_data.json` (frontend visualization data)\n",
    "- `checkpoint_best.pt` (fine-tuned model)\n",
    "- `checkpoint_best.heritage.json` (heritage metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e76f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile, os, glob\n",
    "\n",
    "OUTPUT_ZIP = \"bdh_results.zip\"\n",
    "\n",
    "# Collect files to package\n",
    "files_to_zip = []\n",
    "\n",
    "# merge_data.json (frontend viz)\n",
    "merge_json = \"frontend/public/merge/merge_data.json\"\n",
    "if os.path.exists(merge_json):\n",
    "    files_to_zip.append(merge_json)\n",
    "\n",
    "# Fine-tuned checkpoint\n",
    "if os.path.exists(FINETUNED_PATH):\n",
    "    files_to_zip.append(FINETUNED_PATH)\n",
    "\n",
    "# Heritage JSON if exists\n",
    "heritage_json = FINETUNED_PATH.replace('.pt', '.heritage.json')\n",
    "if os.path.exists(heritage_json):\n",
    "    files_to_zip.append(heritage_json)\n",
    "\n",
    "# Merged checkpoint heritage\n",
    "merged_heritage = MERGED_PATH.replace('.pt', '.heritage.json')\n",
    "if os.path.exists(merged_heritage):\n",
    "    files_to_zip.append(merged_heritage)\n",
    "\n",
    "# Also include the specialists for the frontend\n",
    "for sp in [FRENCH_PATH, PORTUGUESE_PATH, MERGED_PATH]:\n",
    "    if os.path.exists(sp):\n",
    "        files_to_zip.append(sp)\n",
    "\n",
    "print(f\"Packaging {len(files_to_zip)} files into {OUTPUT_ZIP}:\")\n",
    "with zipfile.ZipFile(OUTPUT_ZIP, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
    "    for fp in files_to_zip:\n",
    "        arcname = fp  # preserve directory structure\n",
    "        zf.write(fp, arcname)\n",
    "        size_mb = os.path.getsize(fp) / 1e6\n",
    "        print(f\"  {arcname} ({size_mb:.1f} MB)\")\n",
    "\n",
    "total_mb = os.path.getsize(OUTPUT_ZIP) / 1e6\n",
    "print(f\"\\n  Created: {OUTPUT_ZIP} ({total_mb:.1f} MB)\")\n",
    "print(\"  Download this file from the Kaggle output panel →\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
