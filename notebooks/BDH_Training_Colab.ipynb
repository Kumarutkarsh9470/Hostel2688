{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üêâ BDH Interpretability Suite ‚Äî Full Training & Merge Pipeline\n",
    "\n",
    "This notebook runs the complete pipeline for the KRITI 2026 AI Interpretability Challenge:\n",
    "\n",
    "1. Download Europarl data (English-French + English-Portuguese)\n",
    "2. Train French specialist model\n",
    "3. Train Portuguese specialist model (same architecture!)\n",
    "4. Merge both into a polyglot model\n",
    "5. Evaluate all three models on both languages\n",
    "6. Generate frontend visualization data\n",
    "\n",
    "**Requirements:** Google Colab with GPU (T4 or better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi\n",
    "import torch\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'CUDA: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo (or upload your zip)\n",
    "# !git clone https://github.com/YOUR_USERNAME/BDH_Pathway-monosemanticity-architecture.git\n",
    "# %cd BDH_Pathway-monosemanticity-architecture\n",
    "\n",
    "# Or if uploading zip:\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()  # Upload your zip\n",
    "# !unzip BDH_Pathway-monosemanticity-architecture.zip\n",
    "# %cd BDH_Pathway-monosemanticity-architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pyyaml numpy torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download Europarl Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python training/download_europarl.py --languages en-fr en-pt --output data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data\n",
    "import os\n",
    "for lang in ['en-fr', 'en-pt']:\n",
    "    for split in ['train.bin', 'val.bin']:\n",
    "        path = f'data/{lang}/{split}'\n",
    "        if os.path.exists(path):\n",
    "            size_mb = os.path.getsize(path) / 1024 / 1024\n",
    "            print(f'  ‚úì {path}: {size_mb:.1f} MB')\n",
    "        else:\n",
    "            print(f'  ‚úó {path}: NOT FOUND')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train French Specialist\n",
    "\n",
    "Architecture: 6 layers, 192 embedding dim, 4 heads, 64√ó MLP multiplier\n",
    "‚Üí 3,072 neurons per head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write French config (matching the checked-in french.yaml)\n",
    "french_config = \"\"\"\n",
    "train_data: \"data/en-fr/train.bin\"\n",
    "val_data: \"data/en-fr/val.bin\"\n",
    "\n",
    "# Model architecture\n",
    "n_layer: 6\n",
    "n_embd: 192\n",
    "n_head: 4\n",
    "mlp_multiplier: 64\n",
    "dropout: 0.1\n",
    "vocab_size: 256\n",
    "\n",
    "# Training\n",
    "batch_size: 16\n",
    "block_size: 256\n",
    "max_iters: 5000\n",
    "learning_rate: 1.0e-3\n",
    "min_lr: 1.0e-4\n",
    "warmup_iters: 500\n",
    "weight_decay: 0.1\n",
    "grad_clip: 1.0\n",
    "gradient_accumulation_steps: 8\n",
    "\n",
    "log_interval: 100\n",
    "eval_interval: 500\n",
    "save_interval: 2500\n",
    "eval_iters: 100\n",
    "\n",
    "output_dir: \"checkpoints\"\n",
    "run_name: \"french_specialist\"\n",
    "\n",
    "device: \"cuda\"\n",
    "dtype: \"bfloat16\"\n",
    "compile_model: false\n",
    "\"\"\"\n",
    "\n",
    "os.makedirs('training/configs', exist_ok=True)\n",
    "with open('training/configs/french_colab.yaml', 'w') as f:\n",
    "    f.write(french_config)\n",
    "print('French config saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train French model\n",
    "!python training/train.py --config training/configs/french_colab.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Portuguese Specialist\n",
    "\n",
    "‚ö†Ô∏è **Architecture MUST match French exactly** ‚Äî same n_layer, n_embd, n_head, mlp_multiplier.\n",
    "This is required for the merge to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Portuguese config ‚Äî identical architecture to French!\n",
    "portuguese_config = \"\"\"\n",
    "train_data: \"data/en-pt/train.bin\"\n",
    "val_data: \"data/en-pt/val.bin\"\n",
    "\n",
    "# MUST MATCH FRENCH MODEL EXACTLY!\n",
    "n_layer: 6\n",
    "n_embd: 192\n",
    "n_head: 4\n",
    "mlp_multiplier: 64\n",
    "dropout: 0.1\n",
    "vocab_size: 256\n",
    "\n",
    "# Training (same schedule)\n",
    "batch_size: 16\n",
    "block_size: 256\n",
    "max_iters: 5000\n",
    "learning_rate: 1.0e-3\n",
    "min_lr: 1.0e-4\n",
    "warmup_iters: 500\n",
    "weight_decay: 0.1\n",
    "grad_clip: 1.0\n",
    "gradient_accumulation_steps: 8\n",
    "\n",
    "log_interval: 100\n",
    "eval_interval: 500\n",
    "save_interval: 2500\n",
    "eval_iters: 100\n",
    "\n",
    "output_dir: \"checkpoints\"\n",
    "run_name: \"portuguese_specialist\"\n",
    "\n",
    "device: \"cuda\"\n",
    "dtype: \"bfloat16\"\n",
    "compile_model: false\n",
    "\"\"\"\n",
    "\n",
    "with open('training/configs/portuguese_colab.yaml', 'w') as f:\n",
    "    f.write(portuguese_config)\n",
    "print('Portuguese config saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Portuguese model\n",
    "!python training/train.py --config training/configs/portuguese_colab.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Merge Models + Evaluate + Generate Samples\n",
    "\n",
    "This single command:\n",
    "1. Loads both specialists\n",
    "2. Verifies they're compatible\n",
    "3. Concatenates neuron spaces (N ‚Üí 2N)\n",
    "4. Averages embeddings and lm_head\n",
    "5. Validates the merged model\n",
    "6. Evaluates on both language test sets\n",
    "7. Generates sample text\n",
    "8. Outputs merge_data.json for the frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python analysis/merge.py \\\n",
    "    --model1 checkpoints/french_specialist/checkpoint_best.pt \\\n",
    "    --model2 checkpoints/portuguese_specialist/checkpoint_best.pt \\\n",
    "    --output checkpoints/merged_polyglot.pt \\\n",
    "    --name1 french \\\n",
    "    --name2 portuguese \\\n",
    "    --french-val data/en-fr/val.bin \\\n",
    "    --portuguese-val data/en-pt/val.bin \\\n",
    "    --frontend-json frontend/public/merge/merge_data.json \\\n",
    "    --device cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Verify Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load and display merge results\n",
    "with open('frontend/public/merge/merge_data.json') as f:\n",
    "    merge_data = json.load(f)\n",
    "\n",
    "print('\\nüìä Model Info:')\n",
    "for name, info in merge_data['models'].items():\n",
    "    print(f\"  {info['flag']} {info['name']}: {info['n_neurons']} neurons/head, {info['params']:,} params\")\n",
    "\n",
    "print('\\nüìä Evaluation (next-byte loss, lower = better):')\n",
    "print(f\"  {'Model':<20} {'French':>10} {'Portuguese':>12}\")\n",
    "print(f\"  {'‚îÄ'*44}\")\n",
    "for name, ev in merge_data['evaluation'].items():\n",
    "    fr = f\"{ev['french_loss']:.4f}\" if ev['french_loss'] else '‚Äî'\n",
    "    pt = f\"{ev['portuguese_loss']:.4f}\" if ev['portuguese_loss'] else '‚Äî'\n",
    "    print(f\"  {name:<20} {fr:>10} {pt:>12}\")\n",
    "\n",
    "print('\\nüìù Samples:')\n",
    "for s in merge_data['samples']:\n",
    "    print(f\"  [{s['label']}]\")\n",
    "    print(f\"  {s['generated'][:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Monosemanticity Data (for French model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/precompute_monosemanticity.py \\\n",
    "    --model checkpoints/french_specialist/checkpoint_best.pt \\\n",
    "    --output frontend/public/monosemanticity/precomputed.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Download Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package results for download\n",
    "!zip -r bdh_results.zip \\\n",
    "    checkpoints/french_specialist/checkpoint_best.pt \\\n",
    "    checkpoints/portuguese_specialist/checkpoint_best.pt \\\n",
    "    checkpoints/merged_polyglot.pt \\\n",
    "    checkpoints/merged_polyglot.heritage.json \\\n",
    "    frontend/public/merge/ \\\n",
    "    frontend/public/monosemanticity/ \\\n",
    "    2>/dev/null\n",
    "\n",
    "import os\n",
    "size = os.path.getsize('bdh_results.zip') / 1024 / 1024\n",
    "print(f'\\nüì¶ bdh_results.zip: {size:.1f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download (Colab)\n",
    "from google.colab import files\n",
    "files.download('bdh_results.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done! üéâ\n",
    "\n",
    "**Next steps:**\n",
    "1. Download and extract `bdh_results.zip`\n",
    "2. Copy `frontend/public/merge/` and `frontend/public/monosemanticity/` to your local project\n",
    "3. Run the frontend: `cd frontend && npm install && npm run dev`\n",
    "4. Open http://localhost:5173 and explore!\n",
    "\n",
    "The **Merge** page will now show real data from your trained models."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
