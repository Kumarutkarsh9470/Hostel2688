{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BDH Full Pipeline — Train + Merge + Evaluate\n",
    "\n",
    "**Platform:** Kaggle GPU T4 x2  |  **Est. time:** ~45 min\n",
    "\n",
    "### Before running — 3 things:\n",
    "1. Upload project zip as Kaggle Dataset named `bdh-project`\n",
    "2. Add Data (right sidebar) -> search `bdh-project` -> Add\n",
    "3. Settings: GPU T4 x2 ON, Internet ON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"trusted": true},
   "outputs": [],
   "source": [
    "# === Step 0: Verify GPU ===\n",
    "import torch, os, sys, shutil, json, glob\n",
    "from pathlib import Path\n",
    "\n",
    "print('PyTorch:', torch.__version__)\n",
    "print('CUDA:', torch.cuda.is_available())\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError('No GPU! Settings -> Accelerator -> GPU T4 x2, then restart.')\n",
    "\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    name = torch.cuda.get_device_name(i)\n",
    "    # Use mem_get_info (works on all PyTorch versions with CUDA)\n",
    "    free, total = torch.cuda.mem_get_info(i)\n",
    "    print(f'  GPU {i}: {name} ({total / 1e9:.1f} GB)')\n",
    "\n",
    "print('\\nGPU ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"trusted": true},
   "outputs": [],
   "source": [
    "# === Step 1: Setup project files ===\n",
    "WORK = Path('/kaggle/working')\n",
    "PROJECT = WORK / 'bdh'\n",
    "\n",
    "# Find project in any attached dataset\n",
    "candidates = list(Path('/kaggle/input').rglob('training/bdh.py'))\n",
    "if not candidates:\n",
    "    print('Datasets found in /kaggle/input/:')\n",
    "    for d in Path('/kaggle/input').iterdir():\n",
    "        print(f'  {d.name}/')\n",
    "        for f in sorted(d.rglob('*'))[:15]:\n",
    "            print(f'    {f.relative_to(d)}')\n",
    "    raise RuntimeError(\n",
    "        'Project not found! Make sure you:\\n'\n",
    "        '  1. Ran create_kaggle_zip.ps1 locally\\n'\n",
    "        '  2. Uploaded zip as Kaggle Dataset named bdh-project\\n'\n",
    "        '  3. Clicked Add Data in sidebar and added it')\n",
    "\n",
    "project_src = candidates[0].parent.parent\n",
    "print(f'Found: {project_src}')\n",
    "\n",
    "# Copy to writable dir (Kaggle input is read-only)\n",
    "if PROJECT.exists():\n",
    "    shutil.rmtree(PROJECT)\n",
    "shutil.copytree(project_src, PROJECT)\n",
    "os.chdir(PROJECT)\n",
    "print(f'Copied to: {PROJECT}')\n",
    "\n",
    "for f in ['training/bdh.py', 'training/train.py',\n",
    "          'training/download_europarl.py', 'analysis/merge.py']:\n",
    "    ok = Path(f).exists()\n",
    "    print(f'  {\"ok\" if ok else \"MISSING\"}: {f}')\n",
    "    if not ok:\n",
    "        raise RuntimeError(f'Missing: {f}')\n",
    "\n",
    "print('\\nProject ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"trusted": true},
   "outputs": [],
   "source": [
    "# === Step 1b: Patch bdh.py for PyTorch 2.6+ ===\n",
    "# PyTorch 2.6+ changed torch.load to default weights_only=True.\n",
    "# bdh.py uses torch.load without weights_only=False, which will crash.\n",
    "# We patch it here since we copied to a writable directory.\n",
    "\n",
    "bdh_path = PROJECT / 'training' / 'bdh.py'\n",
    "bdh_code = bdh_path.read_text()\n",
    "\n",
    "old = 'checkpoint = torch.load(checkpoint_path, map_location=device)'\n",
    "new = 'checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)'\n",
    "\n",
    "if old in bdh_code and new not in bdh_code:\n",
    "    bdh_code = bdh_code.replace(old, new)\n",
    "    bdh_path.write_text(bdh_code)\n",
    "    print('Patched bdh.py: added weights_only=False to torch.load')\n",
    "else:\n",
    "    print('bdh.py already patched or has different code, skipping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"trusted": true},
   "outputs": [],
   "source": [
    "!pip install -q pyyaml tqdm requests 2>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"trusted": true},
   "outputs": [],
   "source": [
    "# === Step 2: Check internet and download data ===\n",
    "import urllib.request\n",
    "try:\n",
    "    urllib.request.urlopen('https://www.google.com', timeout=10)\n",
    "    print('Internet: OK')\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f'No internet ({e}). Settings -> Internet -> On, then restart.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"trusted": true},
   "outputs": [],
   "source": [
    "# Download Europarl (~5-10 min)\n",
    "os.chdir(PROJECT)\n",
    "os.system('python training/download_europarl.py --languages en-fr en-pt --output data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"trusted": true},
   "outputs": [],
   "source": [
    "# Verify data\n",
    "for lang in ['en-fr', 'en-pt']:\n",
    "    for split in ['train.bin', 'val.bin']:\n",
    "        p = Path(f'data/{lang}/{split}')\n",
    "        if p.exists():\n",
    "            print(f'  ok: {p} ({p.stat().st_size / 1024 / 1024:.1f} MB)')\n",
    "        else:\n",
    "            raise RuntimeError(f'Missing: {p}. Re-run download cell above.')\n",
    "print('\\nData ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"trusted": true},
   "outputs": [],
   "source": [
    "# === Step 3: Write training configs ===\n",
    "# Architecture MUST be identical for French and Portuguese (required for merge)\n",
    "\n",
    "CONFIG = \"\"\"train_data: \"{train_data}\"\n",
    "val_data: \"{val_data}\"\n",
    "n_layer: 6\n",
    "n_embd: 192\n",
    "n_head: 4\n",
    "mlp_multiplier: 64\n",
    "dropout: 0.1\n",
    "vocab_size: 256\n",
    "batch_size: 16\n",
    "block_size: 256\n",
    "max_iters: 5000\n",
    "learning_rate: 1.0e-3\n",
    "min_lr: 1.0e-4\n",
    "warmup_iters: 500\n",
    "weight_decay: 0.1\n",
    "grad_clip: 1.0\n",
    "gradient_accumulation_steps: 8\n",
    "log_interval: 100\n",
    "eval_interval: 500\n",
    "save_interval: 2500\n",
    "eval_iters: 100\n",
    "output_dir: \"checkpoints\"\n",
    "run_name: \"{run_name}\"\n",
    "device: \"cuda\"\n",
    "dtype: \"bfloat16\"\n",
    "compile_model: false\n",
    "\"\"\"\n",
    "\n",
    "os.makedirs('training/configs', exist_ok=True)\n",
    "\n",
    "Path('training/configs/french_kaggle.yaml').write_text(\n",
    "    CONFIG.format(train_data='data/en-fr/train.bin',\n",
    "                  val_data='data/en-fr/val.bin',\n",
    "                  run_name='french_specialist'))\n",
    "\n",
    "Path('training/configs/portuguese_kaggle.yaml').write_text(\n",
    "    CONFIG.format(train_data='data/en-pt/train.bin',\n",
    "                  val_data='data/en-pt/val.bin',\n",
    "                  run_name='portuguese_specialist'))\n",
    "\n",
    "print('Architecture: 6L, 192D, 4H, 64x MLP = 3072 neurons/head')\n",
    "print('Configs written')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"trusted": true},
   "outputs": [],
   "source": [
    "# === Step 4a: Train French (~15-20 min) ===\n",
    "os.chdir(PROJECT)\n",
    "os.system('python training/train.py --config training/configs/french_kaggle.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"trusted": true},
   "outputs": [],
   "source": [
    "# === Step 4b: Train Portuguese (~15-20 min) ===\n",
    "os.chdir(PROJECT)\n",
    "os.system('python training/train.py --config training/configs/portuguese_kaggle.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"trusted": true},
   "outputs": [],
   "source": [
    "# === Verify checkpoints ===\n",
    "def find_ckpt(name):\n",
    "    base = Path(f'checkpoints/{name}')\n",
    "    if not base.exists():\n",
    "        return None\n",
    "    for f in ['checkpoint_best.pt', 'checkpoint_latest.pt']:\n",
    "        p = base / f\n",
    "        if p.exists():\n",
    "            return str(p)\n",
    "    pts = sorted(base.glob('checkpoint_*.pt'))\n",
    "    return str(pts[-1]) if pts else None\n",
    "\n",
    "fr_ckpt = find_ckpt('french_specialist')\n",
    "pt_ckpt = find_ckpt('portuguese_specialist')\n",
    "\n",
    "for label, ckpt in [('French', fr_ckpt), ('Portuguese', pt_ckpt)]:\n",
    "    if ckpt:\n",
    "        sz = os.path.getsize(ckpt) / 1024 / 1024\n",
    "        print(f'  {label}: {ckpt} ({sz:.1f} MB)')\n",
    "    else:\n",
    "        print(f'  {label}: NOT FOUND')\n",
    "\n",
    "if not fr_ckpt or not pt_ckpt:\n",
    "    raise RuntimeError('Both checkpoints needed. Check training output above.')\n",
    "print('\\nBoth checkpoints ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"trusted": true},
   "outputs": [],
   "source": [
    "# === Step 5: Merge + Evaluate ===\n",
    "os.chdir(PROJECT)\n",
    "os.makedirs('frontend/public/merge', exist_ok=True)\n",
    "\n",
    "cmd = (\n",
    "    f'python analysis/merge.py'\n",
    "    f' --model1 \"{fr_ckpt}\"'\n",
    "    f' --model2 \"{pt_ckpt}\"'\n",
    "    f' --output checkpoints/merged_polyglot.pt'\n",
    "    f' --name1 french --name2 portuguese'\n",
    "    f' --french-val data/en-fr/val.bin'\n",
    "    f' --portuguese-val data/en-pt/val.bin'\n",
    "    f' --frontend-json frontend/public/merge/merge_data.json'\n",
    "    f' --device cuda'\n",
    ")\n",
    "print(cmd, '\\n')\n",
    "ret = os.system(cmd)\n",
    "if ret != 0:\n",
    "    print(f'\\nMerge exited with code {ret}. Check output above.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"trusted": true},
   "outputs": [],
   "source": [
    "# === Step 6: View Results ===\n",
    "os.chdir(PROJECT)\n",
    "merge_json = Path('frontend/public/merge/merge_data.json')\n",
    "\n",
    "if not merge_json.exists():\n",
    "    print('merge_data.json not found. Merge may have failed.')\n",
    "    print('Check Step 5 output for errors.')\n",
    "else:\n",
    "    data = json.loads(merge_json.read_text())\n",
    "\n",
    "    print('=' * 50)\n",
    "    print('  MERGE RESULTS')\n",
    "    print('=' * 50)\n",
    "\n",
    "    for name, info in data.get('models', {}).items():\n",
    "        print(f\"  {info.get('flag','')} {info.get('name','?'):<20} \"\n",
    "              f\"{info.get('n_neurons',0):>6} N/head  \"\n",
    "              f\"{info.get('params',0):>10,} params\")\n",
    "\n",
    "    ev = data.get('evaluation', {})\n",
    "    if ev:\n",
    "        print(f\"\\n  {'Model':<22} {'Fr Loss':>8} {'Pt Loss':>8}\")\n",
    "        print(f\"  {'-'*40}\")\n",
    "        for name, vals in ev.items():\n",
    "            fr = f\"{vals['french_loss']:.4f}\" if vals.get('french_loss') is not None else '  -'\n",
    "            pt = f\"{vals['portuguese_loss']:.4f}\" if vals.get('portuguese_loss') is not None else '  -'\n",
    "            print(f\"  {name:<22} {fr:>8} {pt:>8}\")\n",
    "\n",
    "    for s in data.get('samples', []):\n",
    "        print(f\"\\n  [{s.get('label','')}]\")\n",
    "        print(f\"  {s.get('generated','')[:120]}...\")\n",
    "\n",
    "    print('\\n' + '=' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"trusted": true},
   "outputs": [],
   "source": [
    "# === Step 7 (Optional): Monosemanticity precompute ===\n",
    "os.chdir(PROJECT)\n",
    "script = Path('scripts/precompute_monosemanticity.py')\n",
    "if script.exists() and fr_ckpt:\n",
    "    os.makedirs('frontend/public/monosemanticity', exist_ok=True)\n",
    "    os.system(\n",
    "        f'python scripts/precompute_monosemanticity.py'\n",
    "        f' --model \"{fr_ckpt}\"'\n",
    "        f' --output frontend/public/monosemanticity/precomputed.json'\n",
    "    )\n",
    "else:\n",
    "    print('Skipping: script or checkpoint not found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"trusted": true},
   "outputs": [],
   "source": [
    "# === Step 8: Package for download ===\n",
    "os.chdir(PROJECT)\n",
    "OUTPUT = Path('/kaggle/working/bdh_output')\n",
    "if OUTPUT.exists():\n",
    "    shutil.rmtree(OUTPUT)\n",
    "\n",
    "to_copy = [\n",
    "    'checkpoints/french_specialist/checkpoint_best.pt',\n",
    "    'checkpoints/french_specialist/checkpoint_latest.pt',\n",
    "    'checkpoints/portuguese_specialist/checkpoint_best.pt',\n",
    "    'checkpoints/portuguese_specialist/checkpoint_latest.pt',\n",
    "    'checkpoints/merged_polyglot.pt',\n",
    "    'checkpoints/merged_polyglot.heritage.json',\n",
    "    'frontend/public/merge/merge_data.json',\n",
    "    'frontend/public/monosemanticity/precomputed.json',\n",
    "]\n",
    "\n",
    "for rel in to_copy:\n",
    "    src = PROJECT / rel\n",
    "    if src.exists():\n",
    "        dst = OUTPUT / rel\n",
    "        dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "        shutil.copy2(src, dst)\n",
    "        print(f'  copied: {rel} ({src.stat().st_size/1024/1024:.1f} MB)')\n",
    "    else:\n",
    "        print(f'  skip:   {rel} (not found)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"trusted": true},
   "outputs": [],
   "source": [
    "# Create zip for download\n",
    "os.chdir('/kaggle/working')\n",
    "os.system('zip -r bdh_results.zip bdh_output/')\n",
    "\n",
    "zp = Path('bdh_results.zip')\n",
    "if zp.exists():\n",
    "    print(f'\\nbdh_results.zip: {zp.stat().st_size/1024/1024:.1f} MB')\n",
    "    print('\\nDONE! To download:')\n",
    "    print('  1. Save Version (top right)')\n",
    "    print('  2. Output tab -> download bdh_results.zip')\n",
    "    print('  3. Unzip, copy checkpoints/ and frontend/ into your project')\n",
    "    print('  4. cd frontend && npm install && npm run dev')\n",
    "else:\n",
    "    print('zip creation failed')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
